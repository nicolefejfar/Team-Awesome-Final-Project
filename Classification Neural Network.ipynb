{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed value for the notebook so results are reproducible\n",
    "from numpy.random import seed\n",
    "seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in the data\n",
    "movies = pd.read_csv('moviesClean.csv')\n",
    "movies_df = movies[[\"original_title\", \"year\", \"duration\"]]\n",
    "\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop text-based columns for the model\n",
    "movies.drop(columns=['original_title', 'genre', 'country', 'language', 'revenue_percent', 'budget',\n",
    "                     'worlwide_gross_income', 'director', 'writer', 'production_company', 'actors'], inplace=True)\n",
    "movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_classification\n",
    "\n",
    "X = movies.drop('success', axis = 1)\n",
    "y = movies['success']\n",
    "\n",
    "y = y.values.reshape(-1, 1)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaler = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale both training and testing data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)\n",
    "y_train_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Step 1: Label-encode data set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "encoded_y = label_encoder.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sequential model\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the first layer. The number of inputs must be equal to the\n",
    "# number of columns\n",
    "from tensorflow.keras.layers import Dense\n",
    "number_inputs = 10481\n",
    "number_hidden_nodes = 10\n",
    "model.add(Dense(units=number_hidden_nodes,\n",
    "                activation='relu', input_dim=number_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the final layer. number_classes is the number of labels to predict.\n",
    "number_classes = 2\n",
    "model.add(Dense(units=number_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use categorical crossentropy for categorical data and mean squared error for regression\n",
    "# Hint: your output layer in this example is using software for logistic regression (categorical)\n",
    "# If your output layer activation was `linear` then you may want to use `mse` for loss\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit (train) the model\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=1000,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantify/Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the testing data\n",
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('classification_neural_network.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data put together for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per the shapes above gathered the test observation counts\n",
    "encoded_predictions = model.predict_classes(X_test_scaled[:1595])\n",
    "prediction_labels = label_encoder.inverse_transform(encoded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing how it looks\n",
    "print(f\"Predicted classes: {prediction_labels}\")\n",
    "print(f\"Actual Labels: {y_test[:1595]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the lists above for Predicted into a DF to merge together\n",
    "predicted_df = pd.DataFrame(prediction_labels)\n",
    "predicted_df.reset_index(inplace=True)\n",
    "predicted_df2 = predicted_df.rename(columns={0:\"Predicted\", 'index':'key'})\n",
    "predicted_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the lists above for Actual into a DF to merge together\n",
    "actual_df = pd.DataFrame(np.concatenate(y_test[:1595]))\n",
    "actual_df.reset_index(inplace=True)\n",
    "actual_df2 =actual_df.rename(columns={0:\"Actual\", 'index':'key'})\n",
    "actual_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing X_test2 data to merge back to the original\n",
    "test_movies_df = X_test\n",
    "test_movies_df.reset_index(inplace=True)\n",
    "test_movies_df.reset_index(inplace=True)\n",
    "test_movies_df2 = test_movies_df.rename(columns={'index':'key','level_0':'key2'}) \n",
    "test_movies_df3 = test_movies_df2[[\"key\", \"key2\"]]\n",
    "test_movies_df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing original movies data\n",
    "movies_df.reset_index(inplace=True)\n",
    "movies_df1 = movies_df.rename(columns={'index':'key2'})\n",
    "# movies_df2 = movies_df1[[\"key2\", \"original_title\", \"year\", \"duration\"]]\n",
    "movies_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the data to get the DF to load to CSV\n",
    "merged_df0 = pd.merge(movies_df1, test_movies_df3, on=\"key2\")\n",
    "merged_df1 = pd.merge(merged_df0, actual_df2, on=\"key\")\n",
    "merged_df2 = pd.merge(merged_df1, predicted_df2, on=\"key\")\n",
    "merged_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_final_df = merged_df2[[\"key\", \"key2\", \"original_title\", \"year\", \"duration\", \"Actual\", \"Predicted\" ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "merged_final_df.to_csv('profit_predict_vs_actual.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model\n",
    "# from tensorflow.keras.models import load_model\n",
    "# voice_model = load_model(\"classification_neural_network.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Model Build On Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data\n",
    "movies2 = pd.read_csv('moviesClean.csv')\n",
    "movies_df = movies2[[\"original_title\", \"year\", \"duration\"]]\n",
    "movies2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categories from: https://www.metacritic.com/about-metascores#:~:text=Metacritic%20designates%20a%20movie%20as,section%20of%20the%20best%20critics..\n",
    "\n",
    "![title](static/theme_pics/metascore.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying top scores in order to standardize\n",
    "# test_df = movies.sort_values(\"metascore\", ascending=False)\n",
    "# test_score_df = test_df['metascore']\n",
    "# test_score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Revenue % Column\n",
    "# movies2['success_score'] = (movies['metascore'] / 100)\n",
    "\n",
    "# movies2.head()\n",
    "\n",
    "# Walk down the dataframe, movie by movie to get metascore categories\n",
    "for index, row in movies2.iterrows():\n",
    "    \n",
    "    if row['metascore'] > 60:\n",
    "        movies2.loc[index, 'Favorable to Great'] = 1\n",
    "    else: \n",
    "        movies2.loc[index, 'Favorable to Great'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop text-based columns for the model\n",
    "movies2.drop(columns=['original_title', 'genre', 'country', 'language', 'revenue_percent', 'metascore',\n",
    "                     'worlwide_gross_income', 'director', 'writer', 'production_company', 'actors'], inplace=True)\n",
    "movies2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_classification\n",
    "\n",
    "X2 = movies2.drop('Favorable to Great', axis = 1)\n",
    "y2 = movies2['Favorable to Great']\n",
    "\n",
    "y2 = y2.values.reshape(-1, 1)\n",
    "\n",
    "print(X2.shape, y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train2.shape, X_test2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaler2 = StandardScaler().fit(X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale both training and testing data\n",
    "X_train_scaled2 = X_scaler2.transform(X_train2)\n",
    "X_test_scaled2 = X_scaler2.transform(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "y_train_categorical2 = to_categorical(y_train2)\n",
    "y_test_categorical2 = to_categorical(y_test2)\n",
    "y_train_categorical2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Step 1: Label-encode data set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y2)\n",
    "encoded_y = label_encoder.transform(y2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sequential model\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model2 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the first layer. The number of inputs must be equal to the\n",
    "# number of columns\n",
    "from tensorflow.keras.layers import Dense\n",
    "number_inputs = 10482\n",
    "number_hidden_nodes = 10\n",
    "model2.add(Dense(units=number_hidden_nodes,\n",
    "                activation='relu', input_dim=number_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the final layer. number_classes is the number of labels to predict.\n",
    "number_classes = 2\n",
    "model2.add(Dense(units=number_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use categorical crossentropy for categorical data and mean squared error for regression\n",
    "# Hint: your output layer in this example is using software for logistic regression (categorical)\n",
    "# If your output layer activation was `linear` then you may want to use `mse` for loss\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit (train) the model\n",
    "model2.fit(\n",
    "    X_train_scaled2,\n",
    "    y_train_categorical2,\n",
    "    epochs=1000,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify/Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the testing data\n",
    "model_loss, model_accuracy = model2.evaluate(\n",
    "    X_test_scaled2, y_test_categorical2, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('classification_neural_network2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather Predictions and Actuals on test data for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per the shapes above gathered the test observation counts\n",
    "encoded_predictions = model2.predict_classes(X_test_scaled2[:1595])\n",
    "prediction_labels = label_encoder.inverse_transform(encoded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing how it looks\n",
    "print(f\"Predicted classes: {prediction_labels}\")\n",
    "print(f\"Actual Labels: {y_test2[:1595]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the lists above for Predicted into a DF to merge together\n",
    "predicted_df = pd.DataFrame(prediction_labels)\n",
    "predicted_df.reset_index(inplace=True)\n",
    "predicted_df2 = predicted_df.rename(columns={0:\"Predicted\", 'index':'key'})\n",
    "predicted_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the lists above for Actual into a DF to merge together\n",
    "actual_df = pd.DataFrame(np.concatenate(y_test2[:1595]))\n",
    "actual_df.reset_index(inplace=True)\n",
    "actual_df2 =actual_df.rename(columns={0:\"Actual\", 'index':'key'})\n",
    "actual_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing X_test2 data to merge back to the original\n",
    "test_movies_df = X_test2\n",
    "test_movies_df.reset_index(inplace=True)\n",
    "test_movies_df.reset_index(inplace=True)\n",
    "test_movies_df2 = test_movies_df.rename(columns={'index':'key','level_0':'key2'}) \n",
    "test_movies_df3 = test_movies_df2[[\"key\", \"key2\"]]\n",
    "test_movies_df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing original movies data\n",
    "movies_df.reset_index(inplace=True)\n",
    "movies_df1 = movies_df.rename(columns={'index':'key2'})\n",
    "# movies_df2 = movies_df1[[\"key2\", \"original_title\", \"year\", \"duration\"]]\n",
    "movies_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the data to get the DF to load to CSV\n",
    "merged_df0 = pd.merge(movies_df1, test_movies_df3, on=\"key2\")\n",
    "merged_df1 = pd.merge(merged_df0, actual_df2, on=\"key\")\n",
    "merged_df2 = pd.merge(merged_df1, predicted_df2, on=\"key\")\n",
    "merged_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_final_df = merged_df2[[\"key\", \"key2\", \"original_title\", \"year\", \"duration\", \"Actual\", \"Predicted\" ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "merged_final_df.to_csv('metascore_predict_vs_actual.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
